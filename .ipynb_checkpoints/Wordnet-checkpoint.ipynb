{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet sandbox\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Real Life you’ll export the words you care about from your XML using XSLT and then read the list into your Python program, and we’ll talk about how to do that below. To start, though, let’s concentrate on learning how Wordet works. We’re writing this in an interface that allows us to break up the code into pieces, which means that in order to run the statements at the bottom of the page, you need to have run at least some of the ones at the top. For example, we import Wordnet at the beginning with `from nltk.corpus import wordnet as wn`, and later code depends on our having done that. If you copy and try to run something below without having done the import, you’ll throw an error.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Create a list of sample words, and examine their synsets. We’ve included four spooky words plus one non-spooky control item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('panic.n.02'),\n",
       "  Synset('scare.n.02'),\n",
       "  Synset('frighten.v.01'),\n",
       "  Synset('daunt.v.01')],\n",
       " [Synset('ghost.n.01'),\n",
       "  Synset('ghostwriter.n.01'),\n",
       "  Synset('ghost.n.03'),\n",
       "  Synset('touch.n.03'),\n",
       "  Synset('ghost.v.01'),\n",
       "  Synset('haunt.v.02'),\n",
       "  Synset('ghost.v.03')],\n",
       " [Synset('fear.n.01'), Synset('frighten.v.01')],\n",
       " [Synset('creep.n.01'), Synset('ghost.n.01'), Synset('spook.v.01')],\n",
       " [Synset('koala.n.01')]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn # import Wordnet and call it just 'wn' for brevity\n",
    "words = ['scare', 'ghost', 'fright', 'spook', 'koala'] # create a list of words to examine\n",
    "synset_list =[wn.synsets(word) for word in words] # get the synsets for each word\n",
    "synset_list # display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the correct synset\n",
    "\n",
    "This part requires human analysis, since although the program can recognize the words, it can’t tell which of the possible meanings a word has at a particular location. If you’re working with a word that isn’t in Wordnet, make a note of that, but there isn’t anything else that you can do, since you can’t add words to Wordnet. Note that the same word may represent different synsets in different locations. For example, ‘scare’ could be a noun in one place and a verb in a different place, and those are different synsets.\n",
    "\n",
    "### First get the definitions of each synset for each word\n",
    "\n",
    "We use a counter (`i`) so that we can see easily which synsets go with the same word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Synset('panic.n.02') sudden mass fear and anxiety over anticipated events\n",
      "0 Synset('scare.n.02') a sudden attack of fear\n",
      "0 Synset('frighten.v.01') cause fear in\n",
      "0 Synset('daunt.v.01') cause to lose courage\n",
      "1 Synset('ghost.n.01') a mental representation of some haunting experience\n",
      "1 Synset('ghostwriter.n.01') a writer who gives the credit of authorship to someone else\n",
      "1 Synset('ghost.n.03') the visible disembodied soul of a dead person\n",
      "1 Synset('touch.n.03') a suggestion of some quality\n",
      "1 Synset('ghost.v.01') move like a ghost\n",
      "1 Synset('haunt.v.02') haunt like a ghost; pursue\n",
      "1 Synset('ghost.v.03') write for someone else\n",
      "2 Synset('fear.n.01') an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)\n",
      "2 Synset('frighten.v.01') cause fear in\n",
      "3 Synset('creep.n.01') someone unpleasantly strange or eccentric\n",
      "3 Synset('ghost.n.01') a mental representation of some haunting experience\n",
      "3 Synset('spook.v.01') frighten or scare, and often provoke into a violent action\n",
      "4 Synset('koala.n.01') sluggish tailless Australian arboreal marsupial with grey furry ears and coat; feeds on eucalyptus leaves and bark\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(synset_list)):\n",
    "    [print(i, item, item.definition()) for item in synset_list[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the appropriate synset for each spooky word _in context_\n",
    "\n",
    "Look at your XML and choose the appropriate synset for each word _in context_. For example, if ‘scare’ occurs as a verb that means ‘cause fear in’ in one place, the synset you‘d choose from above would be `frighten.v.01`. If it occurs as a noun that means ‘a sudden attack of fear’ in another, you‘d choose `scare.n.02`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the synset information back into the XML\n",
    "\n",
    "You can’t write this back into the XML automatically because the same word form in the XML might belong to different synsets in different locations (like the use of ‘scare’ as a verb or as a noun, described above). For that reason, you’ll want to add the synset value manually to the tagged words in your XML. For example, if you have:\n",
    "\n",
    "    <p>He <spooky_word>scared</spooky_word> them.</p>\n",
    "\n",
    "You would expand the markup to:\n",
    "\n",
    "    <p>He <spooky_word synset=\"frighten.v.01\">scared</spooky_word> them.</p>\n",
    "\n",
    "The easiest way to add this type of markup is to load the document into &lt;oXygen/&gt; and do a search and replace to search for the string \n",
    "\n",
    "    <spooky_word\n",
    "\n",
    "and replace it with\n",
    "\n",
    "    <spooky_word synset=\"\"\n",
    "\n",
    "This will write the `@synset` attribute into the start tag with a null value, and you can then use the XPath browser box to find all `<spooky_word>` elements and type in the attribute values. You’ll want to modify your schema so that this new attribute will be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the lemmata for each synset\n",
    "\n",
    "At the moment this is just for curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('scare.n.02') has the following lemmata: ['scare', 'panic_attack']\n",
      "Synset('frighten.v.01') has the following lemmata: ['frighten', 'fright', 'scare', 'affright']\n"
     ]
    }
   ],
   "source": [
    "scare_synsets = [wn.synset('scare.n.02'), wn.synset('frighten.v.01')]\n",
    "for synset in scare_synsets:\n",
    "    print(str(synset) + ' has the following lemmata: ' + str([lemma.name() for lemma in synset.lemmas()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore lexical ambiguity\n",
    "\n",
    "Word forms in your text will belong to zero or more synsets, although an occurrence of a word form will belong to only one synset in a particular context. You can quantify the degree of ambiguity, and thus the extent to which the meaning of the word depends on context, by retrieving the number of synsets for each word form in your data. Here’s how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"scare\" belongs to 4 synsets\n",
      "Word \"ghost\" belongs to 7 synsets\n",
      "Word \"fright\" belongs to 2 synsets\n",
      "Word \"spook\" belongs to 3 synsets\n",
      "Word \"koala\" belongs to 1 synsets\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    synset_count = len(wn.synsets(word))\n",
    "    print('Word \"' + word + '\" belongs to ' + str(synset_count) + ' synsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding is fine for humans, but we want to write these counts back into our XML. We can do that automatically in three steps:\n",
    "\n",
    "1. Use XSLT to export a plain text list of words you’ve tagged (e.g., spooky words) from your XML data files.\n",
    "1. Use Python to create an XML auxiliary document that maps each of those words to its synset count. The Python script will read the exported plain text list, use Wordnet to count the number of synsets associated with each of them, and write the word plus the count into an XML document.\n",
    "1. Use an XSLT _identity transformation_ to write the synset count into the XML as new content. Your XSLT transformation will transform each of your XML data files to itself (that is, the output will be identical to the input), except that it will insert an additional attribute that includes the count of synsets associated with the word form.\n",
    "\n",
    "Here’s how that works:\n",
    "\n",
    "#### Step 1: Export a plain text list of words you’ve tagged (e.g., spooky words)\n",
    "\n",
    "Here’s some original sample XML:\n",
    "\n",
    "    <root>\n",
    "        <p>The <spooky_word>ghost</spooky_word> <spooky_word>scared</spooky_word> \n",
    "        him by giving him a <spooky_word>scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "We manually add the synset markup:\n",
    "\n",
    "    <root>\n",
    "        <p>The <spooky_word synset=\"ghost.n.03\">ghost</spooky_word> \n",
    "        <spooky_word synset=\"frighten.v.01\">scared</spooky_word> \n",
    "        him by giving him a \n",
    "        <spooky_word synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "We then run the following XSLT transformation, outputting plain text:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n",
    "        xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" exclude-result-prefixes=\"xs\" version=\"2.0\">\n",
    "        <xsl:output method=\"text\" indent=\"yes\"/>\n",
    "        <xsl:template match=\"/\">\n",
    "            <xsl:apply-templates select=\"//spooky_word\"/>\n",
    "        </xsl:template>\n",
    "        <xsl:template match=\"spooky_word\">\n",
    "            <xsl:value-of select=\"concat(.,'&#x0a;')\"/>\n",
    "        </xsl:template>\n",
    "    </xsl:stylesheet>\n",
    "\n",
    "Note that the value of the `@method` attribute on the `<xsl:output>` element is \"text\" because we’re creating plain text. We apply templates to the `<spooky_word>` elements, and in the template that matches those elements, we output the value of concatenating the content of the element (the word itself) with a new line character (spelled `&#x0a;`, which is the _numerical character reference_ for a new line). The output looks like:\n",
    "\n",
    "    ghost\n",
    "    scared\n",
    "    scare\n",
    "\n",
    "We can save that to a file (let’s call it “spooky_words.txt”), so that we can access it later with Python.\n",
    "\n",
    "#### Step 2: Access that file with Python and create a new XML file that maps each word form to its synset count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open('spooky_words.txt','r') # open the list of spookey words that we just exported for reading \n",
    "wordlist = infile.read().split() # get the words from the file\n",
    "infile.close() # close the input file, since we’re read it all\n",
    "outfile = open('synset_counts.xml','w') # open a file for writing to hold the output\n",
    "outfile.write('<root>') # create a start tag for the root element in the output XML file\n",
    "for word in wordlist: # create output for each word\n",
    "    synset_count = len(wn.synsets(word)) # for each word, count the number of synsets to which it belongs\n",
    "    outfile.write('<word><form>' + word + '</form><count>' + str(synset_count) + '</count></word>') # write it out\n",
    "outfile.write('</root>') # create the end tag for the root element\n",
    "outfile.close() # close the output file, since we’ve written all the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the output to a file, so we don’t see it here in the notebook, but we can now read it. This is just for human inspection, to make sure that it looks the way we want. It isn’t pretty-printed, but we can still see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<root><word><form>ghost</form><count>7</count></word><word><form>scared</form><count>4</count></word><word><form>scare</form><count>4</count></word></root>\n"
     ]
    }
   ],
   "source": [
    "with open('synset_counts.xml') as infile:\n",
    "    print(infile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: To write the counts back into the XML, use an _identity transformation_, reading in the new count file with the XPath `document()` function\n",
    "\n",
    "Assume that we’ve saved our original XML (with the synsets, but without the counts) as original.xml. It looks like:\n",
    "\n",
    "    <root>\n",
    "        <p>The <spooky_word synset=\"ghost.n.03\">ghost</spooky_word> \n",
    "        <spooky_word synset=\"frighten.v.01\">scared</spooky_word> \n",
    "        him by giving him a \n",
    "        <spooky_word synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "Transform it with the following XSLT:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n",
    "        xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n",
    "        exclude-result-prefixes=\"xs\"\n",
    "        version=\"2.0\">\n",
    "        <xsl:variable name=\"count_file\" as=\"document-node()\" select=\"document('synset_counts.xml')\"/>\n",
    "        <xsl:template match=\"node()|@*\">\n",
    "            <xsl:copy>\n",
    "                <xsl:apply-templates select=\"@*|node()\"/>\n",
    "            </xsl:copy>\n",
    "        </xsl:template>\n",
    "        <xsl:template match=\"spooky_word\">\n",
    "            <xsl:copy>\n",
    "                <xsl:attribute name=\"synset_count\" select=\"$count_file//word[form eq current()]/count\"/>\n",
    "                <xsl:apply-templates select=\"@*|node()\"/>\n",
    "            </xsl:copy>\n",
    "        </xsl:template>\n",
    "    </xsl:stylesheet>\n",
    "\n",
    "The `document()` function opens synset_counts.xml (which we created with Python in Step #2) so that we can access it (using the variable name `$count_file`) while we’re transforming original.xml. The first template is an _identity transformation_, which you can read about at https://en.wikipedia.org/wiki/Identity_transform. When you perform an identity transformation, the identity template transforms everything to itself (that is, the output is a copy of the input), except that you write separate templates only for the bits that you want to change. In this case, we’re changing `<spooky_word>` elements to add a new `@synset_count` attribute, inserting the value it copies from the auxiliary file that we created with Python in the preceding step.\n",
    "\n",
    "Here’s the output of that last transformation:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <root>\n",
    "        <p>The <spooky_word synset_count=\"7\" synset=\"ghost.n.03\">ghost</spooky_word> \n",
    "            <spooky_word synset_count=\"4\" synset=\"frighten.v.01\">scared</spooky_word> \n",
    "            him by giving him a \n",
    "            <spooky_word synset_count=\"4\" synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "We can then calculate the extent of ambiguity for the entire document or for each individual paragraph. We might decide that the ambiguity of a paragraph is the average of all of the `@synset_count` values in that paragraph, so that for the sole paragraph here it would be 5, that is, the sum of the three values (15) divided by the number of values (3). We could graph this with SVG to examine whether there’s a pattern to the ambiguity, that is, whether it’s higher in some locations of the story than in others. We could also look for correlations between, say, the number of spooky words and the degree of ambiguity. Or we could compare stories or authors to see whether one there is any regularity or other pattern in the ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the number of representations of each synset in each document\n",
    "\n",
    "You can use XSLT to determine which synsets are favored in which texts or by which authors or at which periods. Consider the following input document:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <root>\n",
    "        <p>The <spooky_word synset=\"ghost.n.03\">ghost</spooky_word>\n",
    "            <spooky_word synset=\"frighten.v.01\">scared</spooky_word> and <spooky_word\n",
    "                synset=\"frighten.v.01\">frightened</spooky_word> him by giving him a <spooky_word\n",
    "                synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "This has four spooky words representing three different synsets. We can count the number of occurrences of each synset using XSLT:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n",
    "        xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" exclude-result-prefixes=\"xs\" version=\"2.0\">\n",
    "        <xsl:output method=\"xml\" indent=\"yes\"/>\n",
    "        <xsl:variable name=\"root\" select=\"/\"/>\n",
    "        <xsl:template match=\"/\">\n",
    "            <data>\n",
    "                <xsl:for-each select=\"distinct-values(//spooky_word/@synset)\">\n",
    "                    <synset_count>\n",
    "                        <synset>\n",
    "                            <xsl:value-of select=\"current()\"/>\n",
    "                        </synset>\n",
    "                        <count>\n",
    "                            <xsl:value-of select=\"count($root//spooky_word[@synset eq current()])\"/>\n",
    "                        </count>\n",
    "\n",
    "                    </synset_count>\n",
    "                </xsl:for-each>\n",
    "            </data>\n",
    "        </xsl:template>\n",
    "    </xsl:stylesheet>\n",
    "\n",
    "We set a variable called `$root` because when we do `<xsl:for-each>` over distinct values we cut ourselves off from the tree, so if we want to get back to it, we need to access it through that variable. Here we get each distinct `@synset` value and count the number of `<spooky_word>` elements that have a `@synset` attribute with that value. In this case the output is:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <data>\n",
    "       <synset_count>\n",
    "          <synset>ghost.n.03</synset>\n",
    "          <count>1</count>\n",
    "       </synset_count>\n",
    "       <synset_count>\n",
    "          <synset>frighten.v.01</synset>\n",
    "          <count>2</count>\n",
    "       </synset_count>\n",
    "       <synset_count>\n",
    "          <synset>scare.n.02</synset>\n",
    "          <count>1</count>\n",
    "       </synset_count>\n",
    "    </data>\n",
    "\n",
    "We could transform that to HTML to SVG for display. The counts let us ask: do some works or authors show a preference for certain synset expressions of spookiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the richness of the expression of spookiness\n",
    "\n",
    "Since we’ve already assigned a synset to each spooky word in our text, we can count the number of different synsets in the text. Do some writers represent spookiness with a greater range of spooky-related meanings, that is, with more synsets, than other writers? Because texts may be of different length, we might want not just to count the number of different synsets, but to express the value as the result of dividing the number of spooky word instances by the number of distinct synsets. We can do that with XSLT and write the result into the document as metadata, performing another identity transformation and this time just adding the count in a new element. Assume our input is the output of the last operation, that is:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <root>\n",
    "        <p>The <spooky_word synset_count=\"7\" synset=\"ghost.n.03\">ghost</spooky_word> \n",
    "            <spooky_word synset_count=\"4\" synset=\"frighten.v.01\">scared</spooky_word> \n",
    "            him by giving him a \n",
    "            <spooky_word synset_count=\"4\" synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "Apply the following XSLT transformation:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n",
    "        xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" exclude-result-prefixes=\"xs\" version=\"2.0\">\n",
    "        <xsl:output method=\"xml\" indent=\"yes\"/>\n",
    "        <xsl:template match=\"node() | @*\">\n",
    "            <xsl:copy>\n",
    "                <xsl:apply-templates select=\"@* | node()\"/>\n",
    "            </xsl:copy>\n",
    "        </xsl:template>\n",
    "        <xsl:template match=\"root\">\n",
    "            <xsl:copy>\n",
    "                <meta>\n",
    "                    <spookiness_ratio>\n",
    "                        <xsl:value-of\n",
    "                            select=\"count(distinct-values(//spooky_word/@synset)) div count(//spooky_word)\"\n",
    "                        />\n",
    "                    </spookiness_ratio>\n",
    "                </meta>\n",
    "                <xsl:apply-templates/>\n",
    "            </xsl:copy>\n",
    "        </xsl:template>\n",
    "    </xsl:stylesheet>\n",
    "\n",
    "We start with the identity transformaiton, but when we match our root element (which we’ve arbitrarily called `<root>`), before we apply templates (that is, process its contents) we create a new `<meta>` child, which contains a `<spookiness_ratio>` element, and we calculate and insert the value there. In this case it turns out to be 1 because there are three `<spooky_word>` elements and three distinct `@synset` values. The fewer the number of synsets, the lower the value will be. If we use our sample input from above:\n",
    "\n",
    "    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    <root>\n",
    "        <p>The <spooky_word synset=\"ghost.n.03\">ghost</spooky_word>\n",
    "            <spooky_word synset=\"frighten.v.01\">scared</spooky_word> and <spooky_word\n",
    "                synset=\"frighten.v.01\">frightened</spooky_word> him by giving him a <spooky_word\n",
    "                synset=\"scare.n.02\">scare</spooky_word>.</p>\n",
    "    </root>\n",
    "\n",
    "and run the same transformation, the value is 0.75 because there are four spooky words and three distinct synsets.\n",
    "\n",
    "This can be analogized to the _type/token ratio_ in corpus linguistics, where types are _distinct_ items (such as _different_ words in a text) and tokens are the items (such as words in the same text, regardless of whether they’re duplicates of other words that we’ve already seen). A high type/token ratio means that the text is lexically varied, with little repetition of words. A low ratio means a less varied vocabulary. In this case the number of spooky words is our token count, and the number of distinct synsets is our type count. A high type/token ratio means that spookiness is expressed in a wider variety of ways; a value of 1 would mean that no synset is repeated. A low ratio would mean that the variety is less; the value cannot be 0 if there’s any spookiness at all, but the least varied possibility is that there are lots of spooky words, but they all represent that same synset.\n",
    "\n",
    "Type/token ratios are sensitive to text length. This is easiest to see at the extreme: the number of distinct words in a language may be very large, but it isn’t infinite (at least, it isn’t infinite in any real language context), while texts can be arbitrarily long. That means that after your text reaches a certain length, you don’t know any words you haven’t used already, so you have to start repeating. The dependence of type/token ratio on text length means that if you want to compare type/token ratios, you can do that meaningfully only for texts of the same length. For that reason, if you want to compare our spookiness analogy across texts, you should use texts of the same length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the richness of the vocabulary (by writer or by text)\n",
    "\n",
    "Synsets are represented by one or more lemmata, which you can retrieve with the `lemmas()` method, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('ghost.n.01') means \"a mental representation of some haunting experience\" and has 6 lemmata: ['ghost', 'shade', 'spook', 'wraith', 'specter', 'spectre']\n",
      "Synset('ghostwriter.n.01') means \"a writer who gives the credit of authorship to someone else\" and has 2 lemmata: ['ghostwriter', 'ghost']\n",
      "Synset('ghost.n.03') means \"the visible disembodied soul of a dead person\" and has 1 lemmata: ['ghost']\n",
      "Synset('touch.n.03') means \"a suggestion of some quality\" and has 3 lemmata: ['touch', 'trace', 'ghost']\n",
      "Synset('ghost.v.01') means \"move like a ghost\" and has 1 lemmata: ['ghost']\n",
      "Synset('haunt.v.02') means \"haunt like a ghost; pursue\" and has 3 lemmata: ['haunt', 'obsess', 'ghost']\n",
      "Synset('ghost.v.03') means \"write for someone else\" and has 2 lemmata: ['ghost', 'ghostwrite']\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets('ghost')\n",
    "for synset in synsets:\n",
    "    lemmata = synset.lemmas()\n",
    "    print(str(synset) + ' means \"' + synset.definition() + '\" and has ' + str(len(lemmata)) + ' lemmata: ' + \\\n",
    "         str([lemma.name() for lemma in lemmata]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `name()` method to get just the lexical part of the lemma.\n",
    "\n",
    "A writer or text that uses the synset 'ghost.n.01' has six lemmata available to express that meaning. What proportion of the available vocabulary does your writer or text use? \n",
    "\n",
    "That would be easy to calculate if the writer always used the exact form provided by the `name()` method of lemmata. You might find that a particular text contains the following mappings of lemmata and word forms:\n",
    "\n",
    "Synset | Word form\n",
    "--- | ---\n",
    "ghost.n.01 | ghost\n",
    "ghost.n.01 | shade\n",
    "ghost.n.01 | spook\n",
    "\n",
    "You can count up the number of word forms associated with each synset, and because each word form corresponds to a different one of the 6 lemmata for that synset, you’ll determine correctly that the writer or text uses 50% of the available lemmata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "available = [lemma.name() for lemma in wn.synset('ghost.n.01').lemmas()]\n",
    "used = ['ghost', 'shade', 'spook']\n",
    "print(len(used) / len(available))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But suppose the forms include different inflections of the same lemma, such as singular 'ghost' and plural 'ghosts'. The challenge here is that those two forms represent the same lemma, so you can’t simply count forms and use that as a surrogate for counting lemmata. Wordnet helps resolve these situations with `wn.morphy()`, which lemmatizes (we’ve added the numbers just to make the output easier to read):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ghost\n",
      "2 ghost\n"
     ]
    }
   ],
   "source": [
    "print(1, wn.morphy('ghost'))\n",
    "print(2, wn.morphy('ghosts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can resolve that variation along the following lines. Here we have the same list as above, except that instead of three items in our `used` variable that correspond to three different lemmata, we have three items that correspond to only two lemmata. Here we print the values of `used` and `normalized` to show that they have the same length, but `normalized` has only two distinct values, while `used` has three. We then convert `normalized` from a list (which allows duplicates) to a set (which doesn’t), which is a quick way of removing duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ghost', 'ghosts', 'spook']\n",
      "['ghost', 'ghost', 'spook']\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "available = [lemma.name() for lemma in wn.synset('ghost.n.01').lemmas()]\n",
    "used = ['ghost', 'ghosts', 'spook']\n",
    "normalized = [wn.morphy(item) for item in used]\n",
    "print(used)\n",
    "print(normalized)\n",
    "print(len(set(normalized)) / len(available))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `wn.morphy()`, then, we can determine the richness of the vocabulary (the number of available different lemmata that are actually used) without being misled by different inflected forms of the same lexeme. Of course we still have to decide how to use these counts to explore or present information about how much of the available vocabulary variation the writer or text actually uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words and phrases\n",
    "\n",
    "Wordnet is primarily about words, and it contains little information about phrases. To the extent that spookiness is expressed in a phrase, if the spooky quality of the phrase depends on a particular word, it may be more useful to tag the word than the entire phrase, since you can look up the word in Wordnet. But in the case of idioms and other phrasal expressions, the spooky quality may not belong to any single word, and in that case you have to tag the entire phrase. You won’t find spooky phrases in Wordnet, but you can use XSLT to determine how much spooky meaning is expressed at a phrasal level that cannot be regarded as obtaining its spookiness from specific individual words. The XSLT for this is so easy that we won’t write out the code; you can retrieve all of the `<spooky_word>` elements and distinguish the ones that are phrases from the ones that aren’t by filtering them with `matches()`, along the lines of:\n",
    "\n",
    "    //spooky_word[matches(., '\\s')]\n",
    "\n",
    "This retrieves all `<spooky_word>` elements and filters them to retain only the ones that match a regex pattern that includes a white space character ('\\s'). You can read about the `matches()` function in Michael Kay. We use `matches()` instead of `contains()` because `contains()` looks at strings, and the words of a phrase may usually be separated by a space character, but they could be separated by a new line character, which isn’t the same string as a space character. But because `matches()` uses regex where `contains()` uses strings, `matches()` can ask “does this item contain any white space character, whether it’s a space character or a new line?”\n",
    "\n",
    "How you use this information is up to you, but it will tell you how much a writer or text depends on spooky words, and how much the spookiness is conveyed at a phrasal, rather than lexical level."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
